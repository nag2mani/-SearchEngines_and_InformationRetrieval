{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body(url):\n",
    "    # This function will print the content of the body.\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    texts = []\n",
    "    start_index = html_content.find(\"<body\")\n",
    "\n",
    "    if start_index != -1:\n",
    "        start_index = html_content.find(\">\", start_index) + 1\n",
    "        while start_index != -1:\n",
    "            end_index = html_content.find(\"<\", start_index)\n",
    "            if end_index != -1:\n",
    "                text = html_content[start_index:end_index].strip()\n",
    "                if text: #text.isalnum()\n",
    "                    texts.append(text)\n",
    "                start_index = html_content.find(\">\", end_index) + 1\n",
    "            else:\n",
    "                break\n",
    "    # This will remove the unwanted things from the texts and add to the result.\n",
    "    result = ''\n",
    "    for i in texts:\n",
    "        if (i[0] not in ['.', ',','', ' ', '(', ')', '[', ']', '{', '}', '@', \"'\", '&', '#', '^']):\n",
    "            if i[0:3] not in ['-->', 'htt', 'jQu']:\n",
    "                # if i.isalnum():\n",
    "                    result = result + i + ' '\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams_and_frequency(web_texts, n):\n",
    "    # This will lower the \n",
    "    word_list = [word.lower() for word in web_texts.split()]\n",
    "\n",
    "    ngrams = {}\n",
    "    for i in range(len(word_list) - n + 1):\n",
    "        ngram = tuple(word_list[i:i + n])\n",
    "        if ngram in ngrams:\n",
    "            ngrams[ngram] += 1\n",
    "        else:\n",
    "            ngrams[ngram] = 1\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_hash_64bit(words_freq):\n",
    "\n",
    "    hashes =  words_freq\n",
    "\n",
    "    p = 53\n",
    "    m = 2**64\n",
    "    \n",
    "    for tuple_of_words in hashes:\n",
    "        hash_val = 0\n",
    "        for word in tuple_of_words:\n",
    "            for i, char in enumerate(word):\n",
    "                hash_val += ord(char) * (p ** i)\n",
    "        \n",
    "        b = bin(hash_val % m)\n",
    "        x = (64 - len(b) + 2) * \"0\" + b[2:]\n",
    "        hashes[tuple_of_words] = x\n",
    "    \n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_simhash_for_doc(ngrams):\n",
    "    # Combine word hashes into a single Simhash value\n",
    "    final_vector = ''\n",
    "    for i in range(64):\n",
    "        vec = ''\n",
    "        for word_hash in ngrams:\n",
    "            vec = vec + ngrams[word_hash][i]\n",
    "        \n",
    "        if vec.count('0') < vec.count('1'):\n",
    "            final_vector = final_vector + '1'\n",
    "        else:\n",
    "            final_vector = final_vector + '0'\n",
    "\n",
    "    return final_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_doc():\n",
    "    doc_url_1 = input(\"Enter Url 1 :\")\n",
    "    doc_url_2 = input(\"Enter Url 2 :\")\n",
    "    n = int(input(\"Enter n-gram value :\"))\n",
    "\n",
    "    web_texts_1 = body(doc_url_1)\n",
    "    web_texts_2 = body(doc_url_2)\n",
    "\n",
    "    f1 = generate_ngrams_and_frequency(web_texts_1, n)\n",
    "    h1 = polynomial_hash_64bit(f1)\n",
    "    v1 = compute_simhash_for_doc(h1)\n",
    "    print(v1)\n",
    "\n",
    "    f2 = generate_ngrams_and_frequency(web_texts_2, n)\n",
    "    h2 = polynomial_hash_64bit(f2)\n",
    "    v2 = compute_simhash_for_doc(h2)\n",
    "    print(v2)\n",
    "\n",
    "    if len(v1) != len(v2):\n",
    "        raise ValueError(\"Lengths of v1 and v2 must be the same\")\n",
    "\n",
    "    total_bits = len(v1)\n",
    "    matching_bits = sum(1 for bit1, bit2 in zip(v1, v2) if bit1 == bit2)\n",
    "\n",
    "    match_percentage = (matching_bits / total_bits) * 100\n",
    "\n",
    "    return match_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000000000000000000000100000110101101100100110101010000111\n",
      "0000000000000000000000000000000000100001001011000000010100100111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = https://en.wikipedia.org/wiki/Machine_learning\n",
    "# b = https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "# c = https://timesofindia.indiatimes.com/india/pm-modi-chairs-brainstorming-session-for-viksit-bharat-2047-heres-what-was-discussed/articleshow/108183149.cms\n",
    "\n",
    "compare_two_doc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
